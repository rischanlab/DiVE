\documentclass[fleqn]{article}
\usepackage[left=1in, right=1in, top=1in, bottom=1in]{geometry}
\usepackage{mathexam}
\usepackage{amsmath}

%\ExamClass{Sample Class}
%\ExamName{Sample Exam}
%\ExamHead{\today}

\let\ds\displaystyle

\begin{document}
%\ExamInstrBox{
%Please show \textbf{all} your work! Answers without supporting work will not be given credit. Write answers in spaces provided. You have 1 hour and 50 minutes to complete this exam.}
%\ExamNameLine
%\begin{enumerate}
%   \item Calculate the following limits.  If a limit is $\infty$ or $-\infty$,
%      please say so.  Make sure you show all your work and justify all your
%      answers.  
%      \begin{enumerate}
%	 \item $\ds{\lim_{x\rightarrow3}\frac{\sqrt{x+1} - 2}{x-3}}$\answer
%	 \item $\ds{\lim_{x\rightarrow0}\frac{\sin(4x)}{8x}}$\answer
%      \end{enumerate}
%   \item Use the $\varepsilon$-$\delta$ definition of limit to prove that 
%      \[\lim_{x\rightarrow 2} x^2 - 3x + 2 = 0\]\noanswer[2.5in]
%      \newpage
%   \item If $h(x) = \sqrt{x^2 + 2} - 1$, find a \textbf{non-trivial} decomposition of $h$ into $f$ and $g$ such that $h = f\circ g$.
%      \answer*{$f(x)=$}\addanswer*{$g(x)=$}
%   \item Find the first two derivatives of the function $f(x) = x^2\cos(x)$.  Simplify
%      your answers as much as possible.  Show all your work.
%      \answer*{$f'(x)=$}\answer*{$f''(x)=$}
%      \newpage
%   \item Find the derivative of the function $\ds{f(x) = \int_{x^2}^2
%      \frac{\cos(t)}{t} \,dt}$.\answer[1in plus 1fill]
%   \item Set up, but do not evaluate, the integral for the volume of the solid obtained by rotating the area between the curves $y = x$ and $y = \sqrt{x}$ about the $x$-axis.\noanswer
%\end{enumerate}

\textbf{Maximum bound of Euclidean distance between two probability distributions}
\newline

Example maximum condition for two bins case: 
\newline

$\sum a = \sum b = 1$, $a , b \geq 0$
\newline

$(\sum a)^2 + (\sum b)^2 \geq \sum a^2 + \sum b^2$
\newline

$(\sum a)^2 + (\sum b)^2 \geq \sum a^2 + \sum b^2 - \sum 2ab $ 
\newline

$(\sum a)^2 + (\sum b)^2 \geq \sum (a^2 +  b^2 -  2ab)  $ 
\newline

$(\sum a)^2 + (\sum b)^2 \geq \sum (a-b)^2  $ 
\newline

$1 + 1 \geq \sum (a-b)^2  $ 
\newline

$\sqrt{2} \geq \sqrt{\sum (a-b)^2}  $ 
\newline

For the general case, Euclidean distance $d$ is defined as following: 
$d = \sum{(x-y)^2} = \sum x^2 + \sum y^2 - 2\sum xy$. Given that in probability vectors all values are nonnegative, $d$ is max when the last term is zero, then $d = \sum x^2 + \sum y^2$.

All values are between 0 and 1 (sum up to 1), $\sum x = \sum y = 1$. In such a vector, its theoretical maximum is attained when all its entries are 0 except one which is 1, it is when $\sum x^2 = \sum x$ and $\sum y^2 = \sum y$. It also follows from the above description, that then $\sum xy$ can very easily happen to be zero (since in each vector there is just single nonzero element).
\newline


%a2 + b2 = (a+b)2 - 2ab
%a2-b2 = (a-b)(a+b)
%(a+b)2 = a2+2ab +b2 
%(a-b)2 = a2-2ab +b2 
%(a+b+c)2 = a2 +b2 +c2 +2ab+2bc+2ac



\textbf{Maximum bound of Kullback-Leibler (KL) distance between two probability distributions}
\newline

For distributions which do not have the same support, KL divergence is not bounded. Look at the definition: $KL(P\vert\vert Q) = \int_{-\infty}^{\infty} p(x)\ln\left(\frac{p(x)}{q(x)}\right) dx$

If $ P $ and $ Q $ have not the same support, there exists some point $x'$ where $p(x') \neq 0$ and $q(x') = 0$, making KL go to infinity. Even both distributions have the same support, when one distribution has a much fatter tail than the other. Then:
$$KL(P\vert\vert Q) = \int p(x)\log\left(\frac{p(x)}{q(x)}\right) \,\text{d}x$$
when
$$p(x)=\overbrace{\frac{1}{\pi}\,\frac{1}{1+x^2}}^\text{Cauchy density}\qquad q(x)=\overbrace{\frac{1}{\sqrt{2\pi}}\,\exp\{-x^2/2\}}^\text{Normal density}$$
then
$$KL(P\vert\vert Q) = \int \frac{1}{\pi}\,\frac{1}{1+x^2} \log p(x) \,\text{d}x + \int \frac{1}{\pi}\,\frac{1}{1+x^2} [\log(2\pi)/2+x^2/2]\,\text{d}x$$
and
$$\int \frac{1}{\pi}\,\frac{1}{1+x^2} x^2/2\,\text{d}x=+\infty$$

In conclusion, Kullback-Leibler (KL) is not bounded. For instance, when I implement KL in my code. In some case while the bin does not has its pair in the reference (which means 0), the result are two posibilities: error devided by 0 or $ log\ 0 $ which is undefined. 
\newline

\textbf{Max-sum and Max-min diversification}
\newline

Max-sum is bi-criteria objective function to maximize the sum of the relevance and dissimilarity of the selected set, which can be defined as follows:

\begin{equation}
F\left(S\right) =  \left(1-\lambda\right) * I\left(S\right) + \lambda * f\left(S,D\right)
\label{objectif_function}
\end{equation}

Where, 
$ I\left(S\right)= \sum_{i=1}^{k} \dfrac{I(V_i )}{I_u}, V_i  \in S $ and $ f\left(S,D\right)= \dfrac{1}{k\left(k-1\right)}  \sum_{i=1}^{k} \sum_{j>i}^{k} D\left(V_i,V_j\right) ,V_i,V_j  \in S $
\newline

Meanwhile, Max-min diversification is the bi-criteria objective function that maximize the \textit{minimum} relevance and dissimilarity of the selected set. Based on the work of Gollapudi (An axiometic approach for result diversificaiton), this objective function can be defined as follows: 

\begin{equation}
F\left(S\right) = (1-\lambda) * \underset{u \in S} {\mathrm{min}} \ w\left(u\right)  + \lambda * \underset{u,v \in S} {\mathrm{min}} d\left(u,v\right)
\end{equation}

While Max-min diversification is to maximize the minimum of importance score, I am not sure this approach is relevant or not for our work. 

\end{document}

